{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# --- Core Application Imports ---\n",
    "from config.config import Config\n",
    "from rfiprocessor.utils.logger import get_logger\n",
    "from rfiprocessor.utils.wlak_dir import list_all_file_paths\n",
    "from rfiprocessor.services.markdown_converter import MarkdownConverter, ProcessorType\n",
    "from rfiprocessor.core.agents.document_classifier import DocumentClassifierAgent\n",
    "from rfiprocessor.core.agents.rfi_parser import RfiParserAgent\n",
    "from rfiprocessor.services.chunker import ChunkerService\n",
    "from rfiprocessor.services.vector_store_service import VectorStoreService\n",
    "\n",
    "\n",
    "# --- Database Imports ---\n",
    "from rfiprocessor.db.database import init_db, get_db_session\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from rfiprocessor.db.db_models import Chunk, IngestionStatus\n",
    "from rfiprocessor.models.data_models import RFIJson\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from langchain_core.documents import Document\n",
    "from rfiprocessor.db import db_models\n",
    "\n",
    "# --- Initial Setup ---\n",
    "config = Config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def run_markdown_conversion_pipeline():\n",
    "    \"\"\"\n",
    "    Scans for new files, converts them to markdown, and tracks progress in the database.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Markdown Conversion ---\n",
    "    logger.info(\"--- Starting Markdown Conversion Pipeline ---\")\n",
    "    \n",
    "    # Initialize services\n",
    "    converter = MarkdownConverter()\n",
    "    classifier = DocumentClassifierAgent()\n",
    "    rfi_parser = RfiParserAgent()\n",
    "    chunker = ChunkerService(config.CHUNK_SIZE, config. CHUNK_OVERLAP)\n",
    "    vector_store_service = VectorStoreService()\n",
    "\n",
    "    batch_size: int = 100\n",
    "    \n",
    "    # Get a database session and initialize the handler\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "\n",
    "        # 1. Scan the incoming directory for all files\n",
    "        all_files = list_all_file_paths(config.INCOMING_DATA_PATH)\n",
    "        \n",
    "        # 2. Register all found files in the database if they don't exist\n",
    "        logger.info(f\"Found {len(all_files)} files. Registering new files in the database...\")\n",
    "        for file_path in all_files:\n",
    "            # This will add the document if new, or get the existing one.\n",
    "            db_handler.add_or_get_document(source_filepath=file_path)\n",
    "        \n",
    "        # 3. Fetch only the documents that are pending conversion\n",
    "        pending_docs = db_handler.get_documents_by_status(IngestionStatus.PENDING)\n",
    "        \n",
    "        # Filter out unsupported files\n",
    "        valid_docs = [\n",
    "            doc for doc in pending_docs \n",
    "            if any(doc.source_filename.lower().endswith(ext) for ext in config.VALID_FILE_EXTNS)\n",
    "        ]\n",
    "        \n",
    "        if not valid_docs:\n",
    "            logger.info(\"No new documents to convert. Pipeline finished.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Found {len(valid_docs)} new documents to process.\")\n",
    "\n",
    "        # 4. Process each pending document\n",
    "        for doc in tqdm(valid_docs, desc=\"Converting files to Markdown\"):\n",
    "            try:\n",
    "                logger.info(f\"Processing document: {doc.source_filename} (ID: {doc.id})\")\n",
    "                \n",
    "                # Determine which processor to use\n",
    "                processor_to_use = ProcessorType.MARKITDOWN\n",
    "                if any(doc.source_filename.lower().endswith(ext) for ext in config.UNSTRD_FILE_EXTNS):\n",
    "                    processor_to_use = ProcessorType.UNSTRUCTURED\n",
    "\n",
    "                # Convert the file to markdown\n",
    "                markdown_content, markdown_path = converter.convert_to_markdown(\n",
    "                    file_path=doc.source_filepath,\n",
    "                    processor=processor_to_use\n",
    "                )\n",
    "                \n",
    "                # Move the original raw file to the processed directory\n",
    "                destination_path = os.path.join(config.PROCESSED_DATA_PATH, doc.source_filename)\n",
    "                os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "                shutil.move(doc.source_filepath, destination_path)\n",
    "                \n",
    "                # Update the document record in the database with the new status and paths\n",
    "                updates = {\n",
    "                    \"markdown_filepath\": markdown_path,\n",
    "                    \"processed_filepath\": destination_path,\n",
    "                    \"ingestion_status\": IngestionStatus.MARKDOWN_CONVERTED,\n",
    "                    \"error_message\": None # Clear any previous errors\n",
    "                }\n",
    "                db_handler.update_document(doc.id, updates)\n",
    "                logger.info(f\"Successfully converted and moved document ID {doc.id}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing document ID {doc.id} ('{doc.source_filename}'): {e}\", exc_info=True)\n",
    "                # Update the document record to reflect the failure\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\n",
    "                        \"ingestion_status\": IngestionStatus.FAILED,\n",
    "                        \"error_message\": str(e)\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        # Ensure the database session is closed\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Markdown Conversion Pipeline Finished ---\")\n",
    "\n",
    "    \"\"\"\n",
    "    Scans for markdown_converted files, classify them to RFI/RFO or Supporting Document with grades, and tracks progress in the database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get a database session and initialize the handler\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "\n",
    "        # --- Step 2: Document Classification ---\n",
    "        logger.info(\"--- Starting Document Classification Step ---\")\n",
    "        docs_to_classify = db_handler.get_documents_by_status(IngestionStatus.MARKDOWN_CONVERTED)\n",
    "        \n",
    "        if not docs_to_classify:\n",
    "            logger.info(\"No new documents to classify.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(docs_to_classify)} documents to classify.\")\n",
    "\n",
    "        for doc in tqdm(docs_to_classify, desc=\"Classifying documents\"):\n",
    "            try:\n",
    "                logger.info(f\"Classifying document: {doc.source_filename} (ID: {doc.id})\")\n",
    "                \n",
    "                # Read the markdown content from the file\n",
    "                with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                    markdown_content = f.read()\n",
    "                \n",
    "                # Call the classifier agent\n",
    "                classification = classifier.classify(markdown_content)\n",
    "                \n",
    "                # Update the document in the database\n",
    "                updates = {\n",
    "                    \"document_type\": classification.get(\"document_type\"),\n",
    "                    \"document_grade\": classification.get(\"document_grade\"),\n",
    "                    \"ingestion_status\": IngestionStatus.CLASSIFIED,\n",
    "                    \"error_message\": None\n",
    "                }\n",
    "                db_handler.update_document(doc.id, updates)\n",
    "                logger.info(f\"Successfully classified document ID {doc.id}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error classifying document ID {doc.id}: {e}\", exc_info=True)\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": f\"Classification failed: {e}\"}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Ingestion Pipeline Finished ---\")\n",
    "\n",
    "    \"\"\"\n",
    "    Scans for classified RFI/RFP files and processes them through the RFI parser pipeline.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting RFI Parsing Pipeline ---\")\n",
    "    \n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "\n",
    "        # --- Step 3: RFI/RFP Parsing ---\n",
    "        logger.info(\"--- Starting RFI/RFP Parsing Step ---\")\n",
    "        docs_to_parse = db_handler.get_documents_by_status(IngestionStatus.CLASSIFIED)\n",
    "\n",
    "        if not docs_to_parse:\n",
    "            logger.info(\"No new classified documents to parse.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(docs_to_parse)} documents to potentially parse.\")\n",
    "\n",
    "        for doc in tqdm(docs_to_parse, desc=\"Parsing RFI/RFP documents\"):\n",
    "            try:\n",
    "                # Only process documents that were classified as RFI/RFP\n",
    "                if doc.document_type != \"RFI/RFP\":\n",
    "                    logger.info(f\"Skipping parsing for doc ID {doc.id} (type: {doc.document_type}). Marking as parsed.\")\n",
    "                    db_handler.update_document(doc.id, {\"ingestion_status\": IngestionStatus.PARSED})\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Parsing RFI/RFP document: {doc.source_filename} (ID: {doc.id})\")\n",
    "                \n",
    "                # Read the markdown content\n",
    "                with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                    markdown_content = f.read()\n",
    "                \n",
    "                # Call the RFI Parser Agent\n",
    "                parsed_data = rfi_parser.parse(markdown_content)\n",
    "                \n",
    "                # --- SAVE TO JSON DATA FOLDER ---\n",
    "                output_dir = config.PARSED_JSON_PATH\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                json_filename = os.path.splitext(doc.source_filename)[0] + \".json\"\n",
    "                output_path = os.path.join(output_dir, json_filename)\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(parsed_data, f, indent=2, ensure_ascii=False)\n",
    "                logger.info(f\"Saved parsed output to {output_path}\")\n",
    "                # --- END SAVE SECTION ---\n",
    "\n",
    "                # Validate the structured output with Pydantic\n",
    "                try:\n",
    "                    validated_data = RFIJson.model_validate(parsed_data)\n",
    "                    logger.info(f\"Successfully validated JSON structure for doc ID {doc.id}.\")\n",
    "                except ValidationError as ve:\n",
    "                    # If validation fails, log the specific error and mark the doc as failed\n",
    "                    error_details = f\"Pydantic validation failed for doc ID {doc.id}: {ve}\"\n",
    "                    logger.error(error_details)\n",
    "                    db_handler.update_document(\n",
    "                        doc.id,\n",
    "                        {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_details}\n",
    "                    )\n",
    "                    continue  # Skip to the next document\n",
    "                \n",
    "                # Update the document record with the JSON payload and new status\n",
    "                updates = {\n",
    "                    \"rfi_json_payload\": validated_data.model_dump(),  # Use the validated data\n",
    "                    \"ingestion_status\": IngestionStatus.PARSED,\n",
    "                    \"error_message\": None\n",
    "                }\n",
    "                db_handler.update_document(doc.id, updates)\n",
    "                logger.info(f\"Successfully parsed and stored JSON for document ID {doc.id}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error parsing document ID {doc.id}: {e}\"\n",
    "                logger.error(error_msg, exc_info=True)\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_msg}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Ingestion Pipeline Finished ---\")\n",
    "\n",
    "    \"\"\"\n",
    "    Scans for PARSED files and chunk them.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Chinking Pipeline ---\")\n",
    "\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "        # --- Step 4: Document Chunking ---\n",
    "        logger.info(\"--- Starting Document Chunking Step ---\")\n",
    "        docs_to_chunk = db_handler.get_documents_by_status(IngestionStatus.PARSED)\n",
    "\n",
    "        if not docs_to_chunk:\n",
    "            logger.info(\"No new parsed documents to chunk.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(docs_to_chunk)} documents to chunk.\")\n",
    "\n",
    "        # Get the maximum chunk ID to start assigning new IDs\n",
    "        max_chunk_id = db_session.query(func.max(Chunk.id)).scalar() or 0\n",
    "        logger.info(f\"Starting new chunk IDs from {max_chunk_id + 1}\")\n",
    "        current_chunk_id = max_chunk_id + 1\n",
    "\n",
    "        for doc in tqdm(docs_to_chunk, desc=\"Chunking documents\"):\n",
    "            try:\n",
    "                logger.info(f\"Chunking document: {doc.source_filename} (ID: {doc.id})\")\n",
    "\n",
    "                if doc.document_type == \"RFI/RFP\" and doc.rfi_json_payload:\n",
    "                    markdown_content = \"\"  # Not used for RFI/RFP\n",
    "                    logger.info(\"Chunking as RFI/RFP (using JSON payload)...\")\n",
    "\n",
    "                elif doc.markdown_filepath:\n",
    "                    with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                        markdown_content = f.read()\n",
    "                    logger.info(\"Chunking as supporting document (using markdown content)...\")\n",
    "                else:\n",
    "                    logger.info(\"Skipping: Document is missing required data for chunking.\")\n",
    "                    continue\n",
    "\n",
    "                # Call the chunker service\n",
    "                chunks_data = chunker.create_chunks_for_document(doc, markdown_content)\n",
    "                logger.info(f\"Number of chunks: {len(chunks_data)}\")\n",
    "\n",
    "                if chunks_data:\n",
    "                    # Assign new IDs to chunks starting from current_chunk_id\n",
    "                    for chunk_data in chunks_data:\n",
    "                        chunk_data['id'] = current_chunk_id\n",
    "                        current_chunk_id += 1\n",
    "                    db_handler.add_chunks_to_document(doc.id, chunks_data)\n",
    "                    # Update the document status to CHUNKED\n",
    "                    db_handler.update_document(doc.id, {\"ingestion_status\": IngestionStatus.CHUNKED})\n",
    "                    logger.info(f\"Chunked document ID {doc.id}: {len(chunks_data)} chunks (IDs {chunk_data['id'] - len(chunks_data) + 1} to {chunk_data['id']})\")\n",
    "                    logger.info(f\"Successfully chunked and stored chunks for document ID {doc.id}.\")\n",
    "                    # Add the created chunks to the database\n",
    "                    db_handler.add_chunks_to_document(doc.id, chunks_data)\n",
    "                else:\n",
    "                    logger.warning(f\"No chunks were created for document ID {doc.id}. Moving to next stage.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error chunking document ID {doc.id}: {e}\"\n",
    "                logger.error(error_msg, exc_info=True)\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_msg}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Ingestion Chinking Finished ---\")\n",
    "\n",
    "        \"\"\"\n",
    "    Loads chunked documents from the database and adds them to the ChromaDB vector store.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Vector Store Pipeline ---\")\n",
    "\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "\n",
    "        # Chunked Documents\n",
    "        chunked_docs = db_handler.get_documents_by_status(IngestionStatus.CHUNKED)\n",
    "\n",
    "        # --- Step: Load and Embed Chunks ---\n",
    "        logger.info(\"--- Starting Vector Store Embedding Step ---\")\n",
    "        chunks_to_embed = db_session.query(Chunk).join(\n",
    "            db_models.Document\n",
    "        ).filter(\n",
    "            db_models.Document.ingestion_status == IngestionStatus.CHUNKED\n",
    "        ).all()\n",
    "\n",
    "        if not chunks_to_embed:\n",
    "            logger.info(\"No chunked documents to embed.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Found {len(chunks_to_embed)} chunks to embed.\")\n",
    "\n",
    "        # Convert chunks to LangChain Documents\n",
    "        documents = []\n",
    "        for chunk in tqdm(chunks_to_embed, desc=\"Preparing chunks for vector store\"):\n",
    "            try:\n",
    "                document = Document(\n",
    "                    page_content=chunk.chunk_text,\n",
    "                    metadata=chunk.chunk_metadata,\n",
    "                    id=str(chunk.id)\n",
    "                )\n",
    "                documents.append(document)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error preparing chunk ID {chunk.id}: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "        # Add documents to vector store in batches\n",
    "        try:\n",
    "            logger.info(f\"Adding {len(documents)} chunks to vector store...\")\n",
    "            vector_ids = vector_store_service.add_documents(batch_size=batch_size)\n",
    "            logger.info(f\"Successfully added {len(vector_ids)} chunks to ChromaDB.\")\n",
    "\n",
    "            # Update document statuses to VECTORIZED\n",
    "            for doc in chunked_docs:\n",
    "                doc_id = int(doc.id)\n",
    "                db_handler.update_document(\n",
    "                    doc_id,\n",
    "                    {\"ingestion_status\": IngestionStatus.VECTORIZED}\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding chunks to vector store: {e}\", exc_info=True)\n",
    "            for doc in documents:\n",
    "                chunk_id = int(doc.id)\n",
    "                db_handler.update_document(\n",
    "                    chunk_id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": str(e)}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Vector Store Pipeline Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Application started.\")\n",
    "    \n",
    "    # Ensure the database and tables are created before running the pipeline\n",
    "    init_db()\n",
    "    \n",
    "    # Run the main processing function\n",
    "    run_markdown_conversion_pipeline()\n",
    "    \n",
    "    logger.info(\"Application finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# --- Core Application Imports ---\n",
    "from config.config import Config\n",
    "from rfiprocessor.utils.logger import get_logger\n",
    "from rfiprocessor.core.agents.document_classifier import DocumentClassifierAgent\n",
    "\n",
    "# --- Database Imports ---\n",
    "from rfiprocessor.db.database import init_db, get_db_session\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from rfiprocessor.db.db_models import IngestionStatus\n",
    "\n",
    "# --- Initial Setup ---\n",
    "config = Config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def run_document_classification_pipeline():\n",
    "    \"\"\"\n",
    "    Scans for markdown_converted files, classify them to RFI/RFO or Supporting Document with grades, and tracks progress in the database.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = DocumentClassifierAgent()\n",
    "\n",
    "    # Get a database session and initialize the handler\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "\n",
    "        # --- Step 2: Document Classification ---\n",
    "        logger.info(\"--- Starting Document Classification Step ---\")\n",
    "        docs_to_classify = db_handler.get_documents_by_status(IngestionStatus.MARKDOWN_CONVERTED)\n",
    "\n",
    "        if not docs_to_classify:\n",
    "            logger.info(\"No new documents to classify.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(docs_to_classify)} documents to classify.\")\n",
    "\n",
    "        for doc in tqdm(docs_to_classify, desc=\"Classifying documents\"):\n",
    "            try:\n",
    "                logger.info(f\"Classifying document: {doc.source_filename} (ID: {doc.id})\")\n",
    "                \n",
    "                # Read the markdown content from the file\n",
    "                with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                    markdown_content = f.read()\n",
    "                \n",
    "                # Call the classifier agent\n",
    "                classification = classifier.classify(markdown_content)\n",
    "                \n",
    "                # Update the document in the database\n",
    "                updates = {\n",
    "                    \"document_type\": classification.get(\"document_type\"),\n",
    "                    \"document_grade\": classification.get(\"document_grade\"),\n",
    "                    \"ingestion_status\": IngestionStatus.CLASSIFIED,\n",
    "                    \"error_message\": None\n",
    "                }\n",
    "                db_handler.update_document(doc.id, updates)\n",
    "                logger.info(f\"Successfully classified document ID {doc.id}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error classifying document ID {doc.id}: {e}\", exc_info=True)\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": f\"Classification failed: {e}\"}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "            db_session.close()\n",
    "            logger.info(\"--- Ingestion Pipeline Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Application started.\")\n",
    "    \n",
    "    # Ensure the database and tables are created before running the pipeline\n",
    "    init_db()\n",
    "    \n",
    "    # Run the main processing function\n",
    "    run_document_classification_pipeline()\n",
    "    \n",
    "    logger.info(\"Application finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# --- Core Application Imports ---\n",
    "from config.config import Config\n",
    "from rfiprocessor.utils.logger import get_logger\n",
    "from rfiprocessor.core.agents.rfi_parser import RfiParserAgent\n",
    "\n",
    "# --- Database Imports ---\n",
    "from rfiprocessor.db.database import init_db, get_db_session\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from rfiprocessor.db.db_models import IngestionStatus\n",
    "from rfiprocessor.models.data_models import RFIJson\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- Initial Setup ---\n",
    "config = Config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def run_rfi_parser_pipeline():\n",
    "    \"\"\"\n",
    "    Scans for classified RFI/RFP files and processes them through the RFI parser pipeline.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting RFI Parsing Pipeline ---\")\n",
    "    rfi_parser = RfiParserAgent()\n",
    "    \n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "\n",
    "        # --- Step 3: RFI/RFP Parsing ---\n",
    "        logger.info(\"--- Starting RFI/RFP Parsing Step ---\")\n",
    "        docs_to_parse = db_handler.get_documents_by_status(IngestionStatus.CLASSIFIED)\n",
    "\n",
    "        if not docs_to_parse:\n",
    "            logger.info(\"No new classified documents to parse.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(docs_to_parse)} documents to potentially parse.\")\n",
    "\n",
    "        for doc in tqdm(docs_to_parse, desc=\"Parsing RFI/RFP documents\"):\n",
    "            try:\n",
    "                # Only process documents that were classified as RFI/RFP\n",
    "                if doc.document_type != \"RFI/RFP\":\n",
    "                    logger.info(f\"Skipping parsing for doc ID {doc.id} (type: {doc.document_type}). Marking as parsed.\")\n",
    "                    db_handler.update_document(doc.id, {\"ingestion_status\": IngestionStatus.PARSED})\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Parsing RFI/RFP document: {doc.source_filename} (ID: {doc.id})\")\n",
    "                \n",
    "                # Read the markdown content\n",
    "                with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                    markdown_content = f.read()\n",
    "                \n",
    "                # Call the RFI Parser Agent\n",
    "                parsed_data = rfi_parser.parse(markdown_content)\n",
    "                \n",
    "                # --- SAVE TO JSON DATA FOLDER ---\n",
    "                output_dir = config.PARSED_JSON_PATH\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                json_filename = os.path.splitext(doc.source_filename)[0] + \".json\"\n",
    "                output_path = os.path.join(output_dir, json_filename)\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(parsed_data, f, indent=2, ensure_ascii=False)\n",
    "                logger.info(f\"Saved parsed output to {output_path}\")\n",
    "                # --- END SAVE SECTION ---\n",
    "\n",
    "                # Validate the structured output with Pydantic\n",
    "                try:\n",
    "                    validated_data = RFIJson.model_validate(parsed_data)\n",
    "                    logger.info(f\"Successfully validated JSON structure for doc ID {doc.id}.\")\n",
    "                except ValidationError as ve:\n",
    "                    # If validation fails, log the specific error and mark the doc as failed\n",
    "                    error_details = f\"Pydantic validation failed for doc ID {doc.id}: {ve}\"\n",
    "                    logger.error(error_details)\n",
    "                    db_handler.update_document(\n",
    "                        doc.id,\n",
    "                        {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_details}\n",
    "                    )\n",
    "                    continue  # Skip to the next document\n",
    "                \n",
    "                # Update the document record with the JSON payload and new status\n",
    "                updates = {\n",
    "                    \"rfi_json_payload\": validated_data.model_dump(),  # Use the validated data\n",
    "                    \"ingestion_status\": IngestionStatus.PARSED,\n",
    "                    \"error_message\": None\n",
    "                }\n",
    "                db_handler.update_document(doc.id, updates)\n",
    "                logger.info(f\"Successfully parsed and stored JSON for document ID {doc.id}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error parsing document ID {doc.id}: {e}\"\n",
    "                logger.error(error_msg, exc_info=True)\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_msg}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Ingestion Pipeline Finished ---\")\n",
    "\n",
    "# The if __name__ == \"__main__\": block remains the same\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Application started.\")\n",
    "    init_db()\n",
    "    run_rfi_parser_pipeline()\n",
    "    logger.info(\"Application finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Core Application Imports ---\n",
    "from config.config import Config\n",
    "from rfiprocessor.utils.logger import get_logger\n",
    "from rfiprocessor.services.chunker import ChunkerService\n",
    "\n",
    "# --- Database Imports ---\n",
    "from rfiprocessor.db.database import init_db, get_db_session\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from rfiprocessor.db.db_models import Document, IngestionStatus\n",
    "from rfiprocessor.models.data_models import RFIJson\n",
    "\n",
    "# --- Initial Setup ---\n",
    "config = Config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def run_chunking_pipeline():\n",
    "    \"\"\"\n",
    "    Scans for PARSED files and chunk them.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Chinking Pipeline ---\")\n",
    "\n",
    "    # Initialize services and agents\n",
    "    chunker = ChunkerService(config.CHUNK_SIZE, config. CHUNK_OVERLAP)\n",
    "\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "        # --- Step 4: Document Chunking ---\n",
    "        logger.info(\"--- Starting Document Chunking Step ---\")\n",
    "        docs_to_chunk = db_handler.get_documents_by_status(IngestionStatus.PARSED)\n",
    "\n",
    "        if not docs_to_chunk:\n",
    "            logger.info(\"No new parsed documents to chunk.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(docs_to_chunk)} documents to chunk.\")\n",
    "\n",
    "        for doc in tqdm(docs_to_chunk, desc=\"Chunking documents\"):\n",
    "            try:\n",
    "                logger.info(f\"Chunking document: {doc.source_filename} (ID: {doc.id})\")\n",
    "\n",
    "                if doc.document_type == \"RFI/RFP\" and doc.rfi_json_payload:\n",
    "                    markdown_content = \"\"  # Not used for RFI/RFP\n",
    "                    logger.info(\"Chunking as RFI/RFP (using JSON payload)...\")\n",
    "\n",
    "                elif doc.markdown_filepath:\n",
    "                    with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                        markdown_content = f.read()\n",
    "                    logger.info(\"Chunking as supporting document (using markdown content)...\")\n",
    "                else:\n",
    "                    logger.info(\"Skipping: Document is missing required data for chunking.\")\n",
    "                    continue\n",
    "\n",
    "                # Call the chunker service\n",
    "                chunks_data = chunker.create_chunks_for_document(doc, markdown_content)\n",
    "                logger.info(f\"Number of chunks: {len(chunks_data)}\")\n",
    "\n",
    "                if chunks_data:\n",
    "                    db_handler.add_chunks_to_document(doc.id, chunks_data)\n",
    "                    # Update the document status to CHUNKED\n",
    "                    db_handler.update_document(doc.id, {\"ingestion_status\": IngestionStatus.CHUNKED})\n",
    "                    logger.info(f\"Successfully chunked and stored chunks for document ID {doc.id}.\")\n",
    "                    # Add the created chunks to the database\n",
    "                    db_handler.add_chunks_to_document(doc.id, chunks_data)\n",
    "                else:\n",
    "                    logger.warning(f\"No chunks were created for document ID {doc.id}. Moving to next stage.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error chunking document ID {doc.id}: {e}\"\n",
    "                logger.error(error_msg, exc_info=True)\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_msg}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Ingestion Chinking Finished ---\")\n",
    "\n",
    "# The if __name__ == \"__main__\": block remains the same\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Application started.\")\n",
    "    init_db()\n",
    "    run_chunking_pipeline()\n",
    "    logger.info(\"Application finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ddb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Core Application Imports ---\n",
    "from config.config import Config\n",
    "from rfiprocessor.utils.logger import get_logger\n",
    "from rfiprocessor.services.vector_store_service import VectorStoreService\n",
    "from rfiprocessor.db.database import init_db, get_db_session\n",
    "from rfiprocessor.db.db_models import Chunk, IngestionStatus\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from langchain_core.documents import Document\n",
    "from rfiprocessor.db import db_models\n",
    "\n",
    "# --- Initial Setup ---\n",
    "config = Config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def run_vector_store_pipeline(batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Loads chunked documents from the database and adds them to the ChromaDB vector store.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Vector Store Pipeline ---\")\n",
    "\n",
    "    # Initialize services\n",
    "    vector_store_service = VectorStoreService()\n",
    "\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "        # --- Step: Load and Embed Chunks ---\n",
    "        logger.info(\"--- Starting Vector Store Embedding Step ---\")\n",
    "        chunks_to_embed = db_session.query(Chunk).join(\n",
    "            db_models.Document\n",
    "        ).filter(\n",
    "            db_models.Document.ingestion_status == IngestionStatus.CHUNKED\n",
    "        ).all()\n",
    "\n",
    "        if not chunks_to_embed:\n",
    "            logger.info(\"No chunked documents to embed.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Found {len(chunks_to_embed)} chunks to embed.\")\n",
    "\n",
    "        # Convert chunks to LangChain Documents\n",
    "        documents = []\n",
    "        for chunk in tqdm(chunks_to_embed, desc=\"Preparing chunks for vector store\"):\n",
    "            try:\n",
    "                document = Document(\n",
    "                    page_content=chunk.chunk_text,\n",
    "                    metadata=chunk.chunk_metadata,\n",
    "                    id=str(chunk.id)\n",
    "                )\n",
    "                documents.append(document)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error preparing chunk ID {chunk.id}: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "        # Add documents to vector store in batches\n",
    "        try:\n",
    "            logger.info(f\"Adding {len(documents)} chunks to vector store...\")\n",
    "            vector_ids = vector_store_service.add_documents(batch_size=batch_size)\n",
    "            logger.info(f\"Successfully added {len(vector_ids)} chunks to ChromaDB.\")\n",
    "\n",
    "            # Update document statuses to VECTORIZED\n",
    "            for doc in documents:\n",
    "                chunk_id = int(doc.id)\n",
    "                db_handler.update_document(\n",
    "                    chunk_id,\n",
    "                    {\"ingestion_status\": IngestionStatus.VECTORIZED}\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding chunks to vector store: {e}\", exc_info=True)\n",
    "            for doc in documents:\n",
    "                chunk_id = int(doc.id)\n",
    "                db_handler.update_document(\n",
    "                    chunk_id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": str(e)}\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Vector Store Pipeline Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Application started.\")\n",
    "    init_db()\n",
    "    run_vector_store_pipeline()\n",
    "    logger.info(\"Application finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f31af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f22bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.utils import embedding_functions\n",
    "from rfiprocessor.db.database import get_db_session\n",
    "from rfiprocessor.db.db_models import Chunk\n",
    "\n",
    "# --- Setup OpenAI Embedding Function ---\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Set your OPENAI_API_KEY in the environment!\"\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-3-large\"  # Use \"text-embedding-ada-002\" for cheaper/faster\n",
    ")\n",
    "\n",
    "# --- Setup ChromaDB Persistent Client ---\n",
    "chroma_path = \"data/vector_store/chroma_db\"\n",
    "client = PersistentClient(path=chroma_path)\n",
    "collection = client.get_or_create_collection(\"chunks\", embedding_function=openai_ef)\n",
    "\n",
    "# --- Load all chunks from DB ---\n",
    "db_session_generator = get_db_session()\n",
    "db_session = next(db_session_generator)\n",
    "chunks = db_session.query(Chunk).all()\n",
    "print(f\"Loaded {len(chunks)} chunks from DB.\")\n",
    "\n",
    "# --- Batch and Add Chunks to Vector DB ---\n",
    "BATCH_SIZE = 100\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch = chunks[i:i+BATCH_SIZE]\n",
    "    documents = [c.chunk_text for c in batch]\n",
    "    ids = [str(c.id) for c in batch]\n",
    "    metadatas = [c.chunk_metadata for c in batch]\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    print(f\"Added batch {i//BATCH_SIZE + 1} ({len(documents)} chunks)\")\n",
    "\n",
    "print(\"All chunks embedded and stored in ChromaDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a39584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Core Application Imports ---\n",
    "from config.config import Config\n",
    "from rfiprocessor.utils.logger import get_logger\n",
    "from rfiprocessor.core.agents.rfi_parser import RfiParserAgent\n",
    "\n",
    "# --- Database Imports ---\n",
    "from rfiprocessor.db.database import init_db, get_db_session\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from rfiprocessor.db.db_models import IngestionStatus\n",
    "from rfiprocessor.models.data_models import RFIJson\n",
    "\n",
    "# --- Initial Setup ---\n",
    "config = Config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def run_chunking_pipeline():\n",
    "    \"\"\"\n",
    "    Scans for new files and processes them through the ingestion pipeline.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Ingestion Pipeline ---\")\n",
    "\n",
    "    # Initialize services and agents\n",
    "    chunker = ChunkerService(config.CHUNK_SIZE, config. CHUNK_OVERLAP)\n",
    "\n",
    "    db_session_generator = get_db_session()\n",
    "    db_session = next(db_session_generator)\n",
    "    try:\n",
    "        db_handler = DatabaseHandler(db_session)\n",
    "        # --- Step 4: Document Chunking ---\n",
    "        logger.info(\"--- Starting Document Chunking Step ---\")\n",
    "        docs_to_chunk = db_handler.get_documents_by_status(IngestionStatus.PARSED)\n",
    "\n",
    "        if not docs_to_chunk:\n",
    "            logger.info(\"No new parsed documents to chunk.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(docs_to_chunk)} documents to chunk.\")\n",
    "\n",
    "        for doc in tqdm(docs_to_chunk, desc=\"Chunking documents\"):\n",
    "            try:\n",
    "                logger.info(f\"Chunking document: {doc.source_filename} (ID: {doc.id})\")\n",
    "                \n",
    "                # We need the markdown content for supporting docs.\n",
    "                # RFI docs don't need it for chunking, but it's cleaner to always have it.\n",
    "                with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                    markdown_content = f.read()\n",
    "                    \n",
    "                # Call the chunker service\n",
    "                chunks_data = chunker.create_chunks_for_document(doc, markdown_content)\n",
    "                \n",
    "                if not chunks_data:\n",
    "                    logger.warning(f\"No chunks were created for document ID {doc.id}. Moving to next stage.\")\n",
    "                else:\n",
    "                    # Add the created chunks to the database\n",
    "                    db_handler.add_chunks_to_document(doc.id, chunks_data)\n",
    "                \n",
    "                # Update the document status to CHUNKED\n",
    "                db_handler.update_document(doc.id, {\"ingestion_status\": IngestionStatus.CHUNKED})\n",
    "                logger.info(f\"Successfully chunked and stored chunks for document ID {doc.id}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error chunking document ID {doc.id}: {e}\"\n",
    "                logger.error(error_msg, exc_info=True)\n",
    "                # db_handler.update_document(\n",
    "                #     doc.id,\n",
    "                #     {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_msg}\n",
    "                # )\n",
    "\n",
    "    finally:\n",
    "        db_session.close()\n",
    "        logger.info(\"--- Ingestion Pipeline Finished ---\")\n",
    "\n",
    "# The if __name__ == \"__main__\": block remains the same\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Application started.\")\n",
    "    init_db()\n",
    "    run_ingestion_pipeline()\n",
    "    logger.info(\"Application finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00fadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0722f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:41:48,662 - rfiprocessor.services.vector_store_service - INFO - Loading OpenAI embedding model: text-embedding-3-large\n",
      "2025-07-28 12:41:48,780 - rfiprocessor.services.vector_store_service - INFO - ChromaDB service initialized. Collection 'chunks' at data/vector_store/chroma_db\n",
      "2025-07-28 12:41:48,786 - __main__ - INFO - Application started.\n",
      "2025-07-28 12:41:48,786 - rfiprocessor.db.database - INFO - Initializing database and creating tables if they don't exist...\n",
      "2025-07-28 12:41:48,792 - rfiprocessor.db.database - INFO - Database initialization complete.\n",
      "2025-07-28 12:41:48,809 - rfiprocessor.services.llm_provider - INFO - Initialized Fast LLM Provider: gpt-4-turbo\n",
      "2025-07-28 12:41:48,810 - rfiprocessor.services.prompt_loader - INFO - Loading prompt 'document_classifier' from: rfiprocessor/prompts/document_classifier.txt\n",
      "2025-07-28 12:41:48,811 - rfiprocessor.core.agents.document_classifier - INFO - DocumentClassifierAgent initialized successfully.\n",
      "2025-07-28 12:41:48,820 - rfiprocessor.services.llm_provider - INFO - Initialized Advanced LLM Provider: gpt-4o\n",
      "2025-07-28 12:41:48,821 - rfiprocessor.services.prompt_loader - INFO - Loading prompt 'rfi_parser_summary' from: rfiprocessor/prompts/rfi_parser_summary.txt\n",
      "2025-07-28 12:41:48,822 - rfiprocessor.services.prompt_loader - INFO - Loading prompt 'rfi_parser_chunking' from: rfiprocessor/prompts/rfi_parser_chunking.txt\n",
      "2025-07-28 12:41:48,823 - rfiprocessor.core.agents.rfi_parser - INFO - RfiParserAgent initialized successfully.\n",
      "2025-07-28 12:41:48,823 - rfiprocessor.services.chunker - INFO - ChunkerService initialized with chunk_size=2000 and chunk_overlap=200\n",
      "2025-07-28 12:41:48,825 - rfiprocessor.services.vector_store_service - INFO - Loading OpenAI embedding model: text-embedding-3-large\n",
      "2025-07-28 12:41:48,835 - rfiprocessor.services.vector_store_service - INFO - ChromaDB service initialized. Collection 'chunks' at data/vector_store/chroma_db\n",
      "2025-07-28 12:41:48,836 - __main__ - INFO - Starting full ingestion pipeline\n",
      "2025-07-28 12:41:48,836 - __main__ - INFO - --- Starting Markdown Conversion Pipeline ---\n",
      "2025-07-28 12:41:48,836 - rfiprocessor.utils.wlak_dir - DEBUG - Found file: data/raw/incoming/PVS Item 2.2 - A 2016-Annual-Report-Cogstate.pdf\n",
      "2025-07-28 12:41:48,836 - __main__ - INFO - Found 1 files. Registering new files in the database...\n",
      "2025-07-28 12:41:48,843 - rfiprocessor.services.db_handler - INFO - Added new document 'PVS Item 2.2 - A 2016-Annual-Report-Cogstate.pdf' to database with ID 1.\n",
      "2025-07-28 12:41:48,843 - rfiprocessor.services.db_handler - DEBUG - Querying for documents with status: PENDING\n",
      "2025-07-28 12:41:48,844 - __main__ - INFO - Found 1 new documents to process.\n",
      "Converting files to Markdown:   0%|          | 0/1 [00:00<?, ?it/s]2025-07-28 12:41:48,926 - __main__ - INFO - Processing document: PVS Item 2.2 - A 2016-Annual-Report-Cogstate.pdf (ID: 1)\n",
      "2025-07-28 12:41:48,927 - rfiprocessor.services.markdown_converter - INFO - Converting file: data/raw/incoming/PVS Item 2.2 - A 2016-Annual-Report-Cogstate.pdf with markitdown\n",
      "2025-07-28 12:41:52,877 - rfiprocessor.services.markdown_converter - INFO - Saved markdown to: data/markdown/incoming/PVS Item 2.2 - A 2016-Annual-Report-Cogstate.md\n",
      "2025-07-28 12:41:52,879 - rfiprocessor.services.db_handler - INFO - Updating document ID 1 with: {'markdown_filepath': 'data/markdown/incoming/PVS Item 2.2 - A 2016-Annual-Report-Cogstate.md', 'processed_filepath': 'data/raw/processed/PVS Item 2.2 - A 2016-Annual-Report-Cogstate.pdf', 'ingestion_status': <IngestionStatus.MARKDOWN_CONVERTED: 'MARKDOWN_CONVERTED'>, 'error_message': None}\n",
      "2025-07-28 12:41:52,881 - __main__ - INFO - Successfully converted and moved document ID 1.\n",
      "Converting files to Markdown: 100%|██████████| 1/1 [00:03<00:00,  3.96s/it]\n",
      "2025-07-28 12:41:52,882 - __main__ - INFO - --- Markdown Conversion Pipeline Finished ---\n",
      "2025-07-28 12:41:52,882 - __main__ - INFO - --- Starting Document Classification Step ---\n",
      "2025-07-28 12:41:52,882 - rfiprocessor.services.db_handler - DEBUG - Querying for documents with status: MARKDOWN_CONVERTED\n",
      "2025-07-28 12:41:52,883 - __main__ - INFO - Found 1 documents to classify.\n",
      "Classifying documents:   0%|          | 0/1 [00:00<?, ?it/s]2025-07-28 12:41:52,884 - __main__ - INFO - Classifying document: PVS Item 2.2 - A 2016-Annual-Report-Cogstate.pdf (ID: 1)\n",
      "2025-07-28 12:41:52,884 - rfiprocessor.core.agents.document_classifier - INFO - Attempting to classify document...\n",
      "2025-07-28 12:41:54,695 - rfiprocessor.core.agents.document_classifier - DEBUG - LLM raw response for classification: {\n",
      "  \"document_type\": \"Supporting Document\",\n",
      "  \"document_grade\": \"Annual Report\"\n",
      "}\n",
      "2025-07-28 12:41:54,696 - rfiprocessor.core.agents.document_classifier - INFO - Document classified successfully as: {'document_type': 'Supporting Document', 'document_grade': 'Annual Report'}\n",
      "2025-07-28 12:41:54,700 - rfiprocessor.services.db_handler - INFO - Updating document ID 1 with: {'document_type': 'Supporting Document', 'document_grade': 'Annual Report', 'ingestion_status': <IngestionStatus.CLASSIFIED: 'CLASSIFIED'>, 'error_message': None}\n",
      "2025-07-28 12:41:54,705 - __main__ - INFO - Successfully classified document ID 1.\n",
      "Classifying documents: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "2025-07-28 12:41:54,707 - __main__ - INFO - --- Document Classification Step Finished ---\n",
      "2025-07-28 12:41:54,708 - __main__ - INFO - --- Starting RFI Parsing Pipeline ---\n",
      "2025-07-28 12:41:54,708 - rfiprocessor.services.db_handler - DEBUG - Querying for documents with status: CLASSIFIED\n",
      "2025-07-28 12:41:54,710 - __main__ - INFO - Found 1 documents to potentially parse.\n",
      "Parsing RFI/RFP documents:   0%|          | 0/1 [00:00<?, ?it/s]2025-07-28 12:41:54,711 - __main__ - INFO - Skipping parsing for doc ID 1 (type: Supporting Document). Marking as parsed.\n",
      "2025-07-28 12:41:54,712 - rfiprocessor.services.db_handler - INFO - Updating document ID 1 with: {'ingestion_status': <IngestionStatus.PARSED: 'PARSED'>}\n",
      "Parsing RFI/RFP documents: 100%|██████████| 1/1 [00:00<00:00, 158.91it/s]\n",
      "2025-07-28 12:41:54,718 - __main__ - INFO - --- RFI Parsing Pipeline Finished ---\n",
      "2025-07-28 12:41:54,718 - __main__ - INFO - --- Starting Chunking Pipeline ---\n",
      "2025-07-28 12:41:54,719 - rfiprocessor.services.db_handler - DEBUG - Querying for documents with status: PARSED\n",
      "2025-07-28 12:41:54,720 - __main__ - INFO - Found 1 documents to chunk.\n",
      "2025-07-28 12:41:54,723 - __main__ - INFO - Starting new chunk IDs from 1\n",
      "Chunking documents:   0%|          | 0/1 [00:00<?, ?it/s]2025-07-28 12:41:54,725 - __main__ - INFO - Chunking document: PVS Item 2.2 - A 2016-Annual-Report-Cogstate.pdf (ID: 1)\n",
      "2025-07-28 12:41:54,725 - __main__ - INFO - Chunking as supporting document (using markdown content)...\n",
      "2025-07-28 12:41:54,726 - rfiprocessor.services.chunker - INFO - Creating chunks for document ID 1 (Type: Supporting Document)\n",
      "2025-07-28 12:41:54,733 - rfiprocessor.services.chunker - INFO - Created 92 semantic chunks for document ID 1.\n",
      "2025-07-28 12:41:54,734 - __main__ - INFO - Number of chunks: 92\n",
      "2025-07-28 12:41:54,746 - rfiprocessor.services.db_handler - INFO - Added 92 chunks to document ID 1.\n",
      "2025-07-28 12:41:54,763 - rfiprocessor.services.db_handler - INFO - Updating document ID 1 with: {'ingestion_status': <IngestionStatus.CHUNKED: 'CHUNKED'>}\n",
      "2025-07-28 12:41:54,765 - __main__ - INFO - Chunked document ID 1: 92 chunks\n",
      "2025-07-28 12:41:54,765 - __main__ - INFO - Successfully chunked and stored chunks for document ID 1.\n",
      "Chunking documents: 100%|██████████| 1/1 [00:00<00:00, 24.54it/s]\n",
      "2025-07-28 12:41:54,766 - __main__ - INFO - --- Chunking Pipeline Finished ---\n",
      "2025-07-28 12:41:54,766 - __main__ - INFO - --- Starting Vector Store Pipeline ---\n",
      "2025-07-28 12:41:54,766 - rfiprocessor.services.db_handler - DEBUG - Querying for documents with status: CHUNKED\n",
      "2025-07-28 12:41:54,769 - __main__ - INFO - Found 92 chunks to embed.\n",
      "Preparing chunks for vector store: 100%|██████████| 92/92 [00:00<00:00, 311945.00it/s]\n",
      "2025-07-28 12:41:54,771 - __main__ - INFO - Adding 92 chunks to vector store...\n",
      "2025-07-28 12:41:54,771 - rfiprocessor.services.vector_store_service - INFO - Loading chunks from database...\n",
      "2025-07-28 12:41:54,773 - rfiprocessor.services.vector_store_service - INFO - Loaded 92 chunks from database.\n",
      "2025-07-28 12:41:54,774 - rfiprocessor.services.vector_store_service - INFO - Adding batch 1 (92 chunks)...\n",
      "2025-07-28 12:41:57,476 - rfiprocessor.services.vector_store_service - INFO - Added batch 1 (92 chunks)\n",
      "2025-07-28 12:41:57,478 - rfiprocessor.services.vector_store_service - INFO - Successfully added 92 chunks to ChromaDB.\n",
      "2025-07-28 12:41:57,478 - __main__ - INFO - Successfully added 92 chunks to ChromaDB.\n",
      "2025-07-28 12:41:57,480 - rfiprocessor.services.db_handler - INFO - Updating document ID 1 with: {'ingestion_status': <IngestionStatus.VECTORIZED: 'VECTORIZED'>}\n",
      "2025-07-28 12:41:57,483 - __main__ - INFO - --- Vector Store Pipeline Finished ---\n",
      "2025-07-28 12:41:57,483 - __main__ - INFO - Full ingestion pipeline completed\n",
      "2025-07-28 12:41:57,484 - __main__ - INFO - Application finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pydantic import ValidationError\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Dict, Any, Generator\n",
    "from sqlalchemy import func\n",
    "\n",
    "# --- Core Application Imports ---\n",
    "from config.config import Config\n",
    "from rfiprocessor.utils.logger import get_logger\n",
    "from rfiprocessor.utils.wlak_dir import list_all_file_paths\n",
    "from rfiprocessor.services.markdown_converter import MarkdownConverter, ProcessorType\n",
    "from rfiprocessor.core.agents.document_classifier import DocumentClassifierAgent\n",
    "from rfiprocessor.core.agents.rfi_parser import RfiParserAgent\n",
    "from rfiprocessor.services.chunker import ChunkerService\n",
    "from rfiprocessor.services.vector_store_service import VectorStoreService\n",
    "\n",
    "# --- Database Imports ---\n",
    "from rfiprocessor.db.database import init_db, get_db_session\n",
    "from rfiprocessor.services.db_handler import DatabaseHandler\n",
    "from rfiprocessor.db.db_models import Chunk, IngestionStatus\n",
    "from rfiprocessor.models.data_models import RFIJson\n",
    "from langchain_core.documents import Document\n",
    "from rfiprocessor.db import db_models\n",
    "\n",
    "# --- Initial Setup ---\n",
    "config = Config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class IngestionPipeline:\n",
    "    \"\"\"Main pipeline class that orchestrates the document ingestion process.\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 100):\n",
    "        self.batch_size = batch_size\n",
    "        self.converter = MarkdownConverter()\n",
    "        self.classifier = DocumentClassifierAgent()\n",
    "        self.rfi_parser = RfiParserAgent()\n",
    "        self.chunker = ChunkerService(config.CHUNK_SIZE, config.CHUNK_OVERLAP)\n",
    "        self.vector_store_service = VectorStoreService()\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_db_handler(self) -> Generator[DatabaseHandler, None, None]:\n",
    "        \"\"\"Context manager for database session handling.\"\"\"\n",
    "        db_session_generator = get_db_session()\n",
    "        db_session = next(db_session_generator)\n",
    "        try:\n",
    "            yield DatabaseHandler(db_session)\n",
    "        finally:\n",
    "            db_session.close()\n",
    "    \n",
    "    def run_full_pipeline(self) -> None:\n",
    "        \"\"\"Runs the complete ingestion pipeline.\"\"\"\n",
    "        logger.info(\"Starting full ingestion pipeline\")\n",
    "        \n",
    "        self._run_markdown_conversion()\n",
    "        self._run_document_classification()\n",
    "        self._run_rfi_parsing()\n",
    "        self._run_document_chunking()\n",
    "        self._run_vector_store_embedding()\n",
    "        \n",
    "        logger.info(\"Full ingestion pipeline completed\")\n",
    "    \n",
    "    def _run_markdown_conversion(self) -> None:\n",
    "        \"\"\"Step 1: Convert documents to markdown format.\"\"\"\n",
    "        logger.info(\"--- Starting Markdown Conversion Pipeline ---\")\n",
    "        \n",
    "        with self.get_db_handler() as db_handler:\n",
    "            # Register all files in database\n",
    "            self._register_files_in_database(db_handler)\n",
    "            \n",
    "            # Get pending documents\n",
    "            pending_docs = db_handler.get_documents_by_status(IngestionStatus.PENDING)\n",
    "            valid_docs = self._filter_valid_documents(pending_docs)\n",
    "            \n",
    "            if not valid_docs:\n",
    "                logger.info(\"No new documents to convert. Pipeline finished.\")\n",
    "                return\n",
    "            \n",
    "            logger.info(f\"Found {len(valid_docs)} new documents to process.\")\n",
    "            \n",
    "            # Process each document\n",
    "            for doc in tqdm(valid_docs, desc=\"Converting files to Markdown\"):\n",
    "                self._convert_single_document(db_handler, doc)\n",
    "        \n",
    "        logger.info(\"--- Markdown Conversion Pipeline Finished ---\")\n",
    "    \n",
    "    def _run_document_classification(self) -> None:\n",
    "        \"\"\"Step 2: Classify documents as RFI/RFP or Supporting Document.\"\"\"\n",
    "        logger.info(\"--- Starting Document Classification Step ---\")\n",
    "        \n",
    "        with self.get_db_handler() as db_handler:\n",
    "            docs_to_classify = db_handler.get_documents_by_status(IngestionStatus.MARKDOWN_CONVERTED)\n",
    "            \n",
    "            if not docs_to_classify:\n",
    "                logger.info(\"No new documents to classify.\")\n",
    "                return\n",
    "            \n",
    "            logger.info(f\"Found {len(docs_to_classify)} documents to classify.\")\n",
    "            \n",
    "            for doc in tqdm(docs_to_classify, desc=\"Classifying documents\"):\n",
    "                self._classify_single_document(db_handler, doc)\n",
    "        \n",
    "        logger.info(\"--- Document Classification Step Finished ---\")\n",
    "    \n",
    "    def _run_rfi_parsing(self) -> None:\n",
    "        \"\"\"Step 3: Parse RFI/RFP documents.\"\"\"\n",
    "        logger.info(\"--- Starting RFI Parsing Pipeline ---\")\n",
    "        \n",
    "        with self.get_db_handler() as db_handler:\n",
    "            docs_to_parse = db_handler.get_documents_by_status(IngestionStatus.CLASSIFIED)\n",
    "            \n",
    "            if not docs_to_parse:\n",
    "                logger.info(\"No new classified documents to parse.\")\n",
    "                return\n",
    "            \n",
    "            logger.info(f\"Found {len(docs_to_parse)} documents to potentially parse.\")\n",
    "            \n",
    "            for doc in tqdm(docs_to_parse, desc=\"Parsing RFI/RFP documents\"):\n",
    "                self._parse_single_document(db_handler, doc)\n",
    "        \n",
    "        logger.info(\"--- RFI Parsing Pipeline Finished ---\")\n",
    "    \n",
    "    def _run_document_chunking(self) -> None:\n",
    "        \"\"\"Step 4: Chunk parsed documents.\"\"\"\n",
    "        logger.info(\"--- Starting Chunking Pipeline ---\")\n",
    "        \n",
    "        db_session_generator = get_db_session()\n",
    "        db_session = next(db_session_generator)\n",
    "        try:\n",
    "            db_handler = DatabaseHandler(db_session)\n",
    "            docs_to_chunk = db_handler.get_documents_by_status(IngestionStatus.PARSED)\n",
    "            \n",
    "            if not docs_to_chunk:\n",
    "                logger.info(\"No new parsed documents to chunk.\")\n",
    "                return\n",
    "            \n",
    "            logger.info(f\"Found {len(docs_to_chunk)} documents to chunk.\")\n",
    "            \n",
    "            # Get starting chunk ID\n",
    "            max_chunk_id = db_session.query(func.max(Chunk.id)).scalar() or 0\n",
    "            logger.info(f\"Starting new chunk IDs from {max_chunk_id + 1}\")\n",
    "            current_chunk_id = max_chunk_id + 1\n",
    "            \n",
    "            for doc in tqdm(docs_to_chunk, desc=\"Chunking documents\"):\n",
    "                current_chunk_id = self._chunk_single_document(db_handler, doc, current_chunk_id)\n",
    "        finally:\n",
    "            db_session.close()\n",
    "        \n",
    "        logger.info(\"--- Chunking Pipeline Finished ---\")\n",
    "    \n",
    "    def _run_vector_store_embedding(self) -> None:\n",
    "        \"\"\"Step 5: Embed chunks into vector store.\"\"\"\n",
    "        logger.info(\"--- Starting Vector Store Pipeline ---\")\n",
    "        \n",
    "        db_session_generator = get_db_session()\n",
    "        db_session = next(db_session_generator)\n",
    "        try:\n",
    "            db_handler = DatabaseHandler(db_session)\n",
    "            chunked_docs = db_handler.get_documents_by_status(IngestionStatus.CHUNKED)\n",
    "            \n",
    "            # Get chunks to embed\n",
    "            chunks_to_embed = db_session.query(Chunk).join(\n",
    "                db_models.Document\n",
    "            ).filter(\n",
    "                db_models.Document.ingestion_status == IngestionStatus.CHUNKED\n",
    "            ).all()\n",
    "            \n",
    "            if not chunks_to_embed:\n",
    "                logger.info(\"No chunked documents to embed.\")\n",
    "                return\n",
    "            \n",
    "            logger.info(f\"Found {len(chunks_to_embed)} chunks to embed.\")\n",
    "            \n",
    "            # Convert and embed chunks\n",
    "            documents = self._prepare_chunks_for_embedding(chunks_to_embed)\n",
    "            self._embed_documents_to_vector_store(db_handler, documents, chunked_docs)\n",
    "        finally:\n",
    "            db_session.close()\n",
    "        \n",
    "        logger.info(\"--- Vector Store Pipeline Finished ---\")\n",
    "    \n",
    "    def _register_files_in_database(self, db_handler: DatabaseHandler) -> None:\n",
    "        \"\"\"Register all found files in the database.\"\"\"\n",
    "        all_files = list_all_file_paths(config.INCOMING_DATA_PATH)\n",
    "        logger.info(f\"Found {len(all_files)} files. Registering new files in the database...\")\n",
    "        \n",
    "        for file_path in all_files:\n",
    "            db_handler.add_or_get_document(source_filepath=file_path)\n",
    "    \n",
    "    def _filter_valid_documents(self, pending_docs: List[Any]) -> List[Any]:\n",
    "        \"\"\"Filter out unsupported file types.\"\"\"\n",
    "        return [\n",
    "            doc for doc in pending_docs \n",
    "            if any(doc.source_filename.lower().endswith(ext) for ext in config.VALID_FILE_EXTNS)\n",
    "        ]\n",
    "    \n",
    "    def _convert_single_document(self, db_handler: DatabaseHandler, doc: Any) -> None:\n",
    "        \"\"\"Convert a single document to markdown.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Processing document: {doc.source_filename} (ID: {doc.id})\")\n",
    "            \n",
    "            # Determine processor type\n",
    "            processor_to_use = ProcessorType.MARKITDOWN\n",
    "            if any(doc.source_filename.lower().endswith(ext) for ext in config.UNSTRD_FILE_EXTNS):\n",
    "                processor_to_use = ProcessorType.UNSTRUCTURED\n",
    "            \n",
    "            # Convert to markdown\n",
    "            markdown_content, markdown_path = self.converter.convert_to_markdown(\n",
    "                file_path=doc.source_filepath,\n",
    "                processor=processor_to_use\n",
    "            )\n",
    "            \n",
    "            # Move original file\n",
    "            destination_path = os.path.join(config.PROCESSED_DATA_PATH, doc.source_filename)\n",
    "            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "            shutil.move(doc.source_filepath, destination_path)\n",
    "            \n",
    "            # Update database\n",
    "            updates = {\n",
    "                \"markdown_filepath\": markdown_path,\n",
    "                \"processed_filepath\": destination_path,\n",
    "                \"ingestion_status\": IngestionStatus.MARKDOWN_CONVERTED,\n",
    "                \"error_message\": None\n",
    "            }\n",
    "            db_handler.update_document(doc.id, updates)\n",
    "            logger.info(f\"Successfully converted and moved document ID {doc.id}.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing document ID {doc.id} ('{doc.source_filename}'): {e}\", exc_info=True)\n",
    "            db_handler.update_document(\n",
    "                doc.id,\n",
    "                {\n",
    "                    \"ingestion_status\": IngestionStatus.FAILED,\n",
    "                    \"error_message\": str(e)\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    def _classify_single_document(self, db_handler: DatabaseHandler, doc: Any) -> None:\n",
    "        \"\"\"Classify a single document.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Classifying document: {doc.source_filename} (ID: {doc.id})\")\n",
    "            \n",
    "            # Read markdown content\n",
    "            with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # Classify document\n",
    "            classification = self.classifier.classify(markdown_content)\n",
    "            \n",
    "            # Update database\n",
    "            updates = {\n",
    "                \"document_type\": classification.get(\"document_type\"),\n",
    "                \"document_grade\": classification.get(\"document_grade\"),\n",
    "                \"ingestion_status\": IngestionStatus.CLASSIFIED,\n",
    "                \"error_message\": None\n",
    "            }\n",
    "            db_handler.update_document(doc.id, updates)\n",
    "            logger.info(f\"Successfully classified document ID {doc.id}.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error classifying document ID {doc.id}: {e}\", exc_info=True)\n",
    "            db_handler.update_document(\n",
    "                doc.id,\n",
    "                {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": f\"Classification failed: {e}\"}\n",
    "            )\n",
    "    \n",
    "    def _parse_single_document(self, db_handler: DatabaseHandler, doc: Any) -> None:\n",
    "        \"\"\"Parse a single RFI/RFP document.\"\"\"\n",
    "        try:\n",
    "            # Skip non-RFI/RFP documents\n",
    "            if doc.document_type != \"RFI/RFP\":\n",
    "                logger.info(f\"Skipping parsing for doc ID {doc.id} (type: {doc.document_type}). Marking as parsed.\")\n",
    "                db_handler.update_document(doc.id, {\"ingestion_status\": IngestionStatus.PARSED})\n",
    "                return\n",
    "            \n",
    "            logger.info(f\"Parsing RFI/RFP document: {doc.source_filename} (ID: {doc.id})\")\n",
    "            \n",
    "            # Read markdown content\n",
    "            with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # Parse document\n",
    "            parsed_data = self.rfi_parser.parse(markdown_content)\n",
    "            \n",
    "            # Save to JSON file\n",
    "            self._save_parsed_json(doc, parsed_data)\n",
    "            \n",
    "            # Validate with Pydantic\n",
    "            try:\n",
    "                validated_data = RFIJson.model_validate(parsed_data)\n",
    "                logger.info(f\"Successfully validated JSON structure for doc ID {doc.id}.\")\n",
    "            except ValidationError as ve:\n",
    "                error_details = f\"Pydantic validation failed for doc ID {doc.id}: {ve}\"\n",
    "                logger.error(error_details)\n",
    "                db_handler.update_document(\n",
    "                    doc.id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_details}\n",
    "                )\n",
    "                return\n",
    "            \n",
    "            # Update database\n",
    "            updates = {\n",
    "                \"rfi_json_payload\": validated_data.model_dump(),\n",
    "                \"ingestion_status\": IngestionStatus.PARSED,\n",
    "                \"error_message\": None\n",
    "            }\n",
    "            db_handler.update_document(doc.id, updates)\n",
    "            logger.info(f\"Successfully parsed and stored JSON for document ID {doc.id}.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error parsing document ID {doc.id}: {e}\"\n",
    "            logger.error(error_msg, exc_info=True)\n",
    "            db_handler.update_document(\n",
    "                doc.id,\n",
    "                {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_msg}\n",
    "            )\n",
    "    \n",
    "    def _save_parsed_json(self, doc: Any, parsed_data: Dict[str, Any]) -> None:\n",
    "        \"\"\"Save parsed data to JSON file.\"\"\"\n",
    "        output_dir = config.PARSED_JSON_PATH\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        json_filename = os.path.splitext(doc.source_filename)[0] + \".json\"\n",
    "        output_path = os.path.join(output_dir, json_filename)\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(parsed_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Saved parsed output to {output_path}\")\n",
    "    \n",
    "    def _chunk_single_document(self, db_handler: DatabaseHandler, doc: Any, current_chunk_id: int) -> int:\n",
    "        \"\"\"Chunk a single document and return the next available chunk ID.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Chunking document: {doc.source_filename} (ID: {doc.id})\")\n",
    "            \n",
    "            # Prepare content for chunking\n",
    "            if doc.document_type == \"RFI/RFP\" and doc.rfi_json_payload:\n",
    "                markdown_content = \"\"  # Not used for RFI/RFP\n",
    "                logger.info(\"Chunking as RFI/RFP (using JSON payload)...\")\n",
    "            elif doc.markdown_filepath:\n",
    "                with open(doc.markdown_filepath, 'r', encoding='utf-8') as f:\n",
    "                    markdown_content = f.read()\n",
    "                logger.info(\"Chunking as supporting document (using markdown content)...\")\n",
    "            else:\n",
    "                logger.info(\"Skipping: Document is missing required data for chunking.\")\n",
    "                return current_chunk_id\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks_data = self.chunker.create_chunks_for_document(doc, markdown_content)\n",
    "            logger.info(f\"Number of chunks: {len(chunks_data)}\")\n",
    "            \n",
    "            if chunks_data:\n",
    "                # Assign chunk IDs\n",
    "                for chunk_data in chunks_data:\n",
    "                    chunk_data['id'] = current_chunk_id\n",
    "                    current_chunk_id += 1\n",
    "                \n",
    "                # Save to database\n",
    "                db_handler.add_chunks_to_document(doc.id, chunks_data)\n",
    "                db_handler.update_document(doc.id, {\"ingestion_status\": IngestionStatus.CHUNKED})\n",
    "                \n",
    "                logger.info(f\"Chunked document ID {doc.id}: {len(chunks_data)} chunks\")\n",
    "                logger.info(f\"Successfully chunked and stored chunks for document ID {doc.id}.\")\n",
    "            else:\n",
    "                logger.warning(f\"No chunks were created for document ID {doc.id}. Moving to next stage.\")\n",
    "            \n",
    "            return current_chunk_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error chunking document ID {doc.id}: {e}\"\n",
    "            logger.error(error_msg, exc_info=True)\n",
    "            db_handler.update_document(\n",
    "                doc.id,\n",
    "                {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": error_msg}\n",
    "            )\n",
    "            return current_chunk_id\n",
    "    \n",
    "    def _prepare_chunks_for_embedding(self, chunks_to_embed: List[Chunk]) -> List[Document]:\n",
    "        \"\"\"Convert database chunks to LangChain Documents.\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for chunk in tqdm(chunks_to_embed, desc=\"Preparing chunks for vector store\"):\n",
    "            try:\n",
    "                document = Document(\n",
    "                    page_content=chunk.chunk_text,\n",
    "                    metadata=chunk.chunk_metadata,\n",
    "                    id=str(chunk.id)\n",
    "                )\n",
    "                documents.append(document)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error preparing chunk ID {chunk.id}: {e}\", exc_info=True)\n",
    "                continue\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _embed_documents_to_vector_store(self, db_handler: DatabaseHandler, documents: List[Document], chunked_docs: List[Any]) -> None:\n",
    "        \"\"\"Embed documents to vector store and update database.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Adding {len(documents)} chunks to vector store...\")\n",
    "            vector_ids = self.vector_store_service.add_documents(batch_size=self.batch_size)\n",
    "            logger.info(f\"Successfully added {len(vector_ids)} chunks to ChromaDB.\")\n",
    "            \n",
    "            # Update document statuses\n",
    "            for doc in chunked_docs:\n",
    "                doc_id = int(doc.id)\n",
    "                db_handler.update_document(\n",
    "                    doc_id,\n",
    "                    {\"ingestion_status\": IngestionStatus.VECTORIZED}\n",
    "                )\n",
    "\n",
    "                # Move original file\n",
    "                file_name = doc.markdown_filepath.split(\"/\")[-1]\n",
    "                destination_path = os.path.join(config.PROCESSED_MARKDOWN_PATH, file_name)\n",
    "                os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "                shutil.move(doc.markdown_filepath, destination_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding chunks to vector store: {e}\", exc_info=True)\n",
    "            for doc in documents:\n",
    "                chunk_id = int(doc.id)\n",
    "                db_handler.update_document(\n",
    "                    chunk_id,\n",
    "                    {\"ingestion_status\": IngestionStatus.FAILED, \"error_message\": str(e)}\n",
    "                )\n",
    "\n",
    "\n",
    "def run_ingestion_pipeline(batch_size: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Main entry point for the ingestion pipeline.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of documents to process in each batch\n",
    "    \"\"\"\n",
    "    pipeline = IngestionPipeline(batch_size)\n",
    "    pipeline.run_full_pipeline()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Application started.\")\n",
    "    \n",
    "    # Ensure the database and tables are created before running the pipeline\n",
    "    init_db()\n",
    "    \n",
    "    # Run the main processing function\n",
    "    run_ingestion_pipeline()\n",
    "    \n",
    "    logger.info(\"Application finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1fca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfi_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
